import time
import torch
from tqdm import tqdm

# Move model to GPU if available
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)

# Use MSELoss and scale up the loss value
criterion = torch.nn.MSELoss()  # Using MSE instead of L1Loss
optimizer = torch.optim.SGD(model.parameters(), lr=0.5, momentum=0.9)  # Increased LR
scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=100, gamma=0.9)

num_epochs = 3000
best_valid_loss = float("inf")
patience = 50
counter = 0

for epoch in range(num_epochs):
    model.train()
    train_loss = 0.0
    start_time = time.time()
    
    train_progress = tqdm(train_loader, desc=f"Epoch {epoch+1}/{num_epochs} [Training]")
    for X_batch, y_batch in train_progress:
        X_batch, y_batch = X_batch.to(device), y_batch.to(device)

        optimizer.zero_grad()
        outputs = model(X_batch)

        # Debugging: Print outputs and targets
        if epoch % 100 == 0 and train_progress.n == 0:
            print(f"\nSample Output: {outputs[0].item()}, Target: {y_batch[0].item()}")

        loss = criterion(outputs, y_batch) * 100  # Scale up loss for better updates

        # Debugging: Print gradients before update
        loss.backward()
        for name, param in model.named_parameters():
            if param.grad is not None and epoch % 100 == 0:
                print(f"Gradient Norm {name}: {param.grad.norm()}")

        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)  # Clip gradients
        optimizer.step()

        train_loss += loss.item()
        train_progress.set_postfix({"batch_loss": f"{loss.item():.6f}"})

    # Validation phase
    model.eval()
    valid_loss = 0.0
    with torch.no_grad():
        valid_progress = tqdm(valid_loader, desc=f"Epoch {epoch+1}/{num_epochs} [Validation]")
        for X_batch, y_batch in valid_progress:
            X_batch, y_batch = X_batch.to(device), y_batch.to(device)
            outputs = model(X_batch)
            loss = criterion(outputs, y_batch) * 100  # Scale up validation loss
            valid_loss += loss.item()
            valid_progress.set_postfix({"batch_loss": f"{loss.item():.6f}"})

    avg_train_loss = train_loss / len(train_loader)
    avg_valid_loss = valid_loss / len(valid_loader)

    scheduler.step()
    current_lr = optimizer.param_groups[0]['lr']

    if avg_valid_loss < best_valid_loss:
        improvement = (best_valid_loss - avg_valid_loss) / best_valid_loss * 100
        best_valid_loss = avg_valid_loss
        torch.save(model.state_dict(), "best_qbm_model.pth")
        print(f"✅ Model improved by {improvement:.2f}% - saved to best_qbm_model.pth")
        counter = 0
    else:
        counter += 1
        print(f"⚠️ No improvement for {counter} epochs")

    epoch_time = time.time() - start_time
    print(f"Epoch {epoch+1}/{num_epochs} completed in {epoch_time:.2f}s:")
    print(f"  Train Loss: {avg_train_loss:.6f} | Valid Loss: {avg_valid_loss:.6f} | LR: {current_lr:.8f}")

    if counter >= patience:
        print(f"🚨 Early stopping triggered after {epoch+1} epochs")
        break

    print("-" * 80)
    time.sleep(0.1)

print(f"Training complete. Best validation loss: {best_valid_loss:.6f}")
